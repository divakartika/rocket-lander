{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "<LARGE>Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator</LARGE>"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from google.colab import drive\r\n",
                "drive.mount('/content/gdrive')\r\n",
                "\r\n",
                "os.makedirs(\"/content/gdrive/MyDrive/colab_model/rocket/DDPG/\", exist_ok=True)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Installing required libraries"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "!apt-get --purge remove cuda nvidia* libnvidia-*\r\n",
                "!dpkg -l | grep cuda- | awk '{print $2}' | xargs -n1 dpkg --purge\r\n",
                "!apt-get remove cuda-*\r\n",
                "!apt autoremove\r\n",
                "!apt-get update"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "!wget  --no-clobber https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\r\n",
                "!dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\r\n",
                "!sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\r\n",
                "!apt-get update\r\n",
                "!apt-get install cuda-10-0"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "!apt install python-opengl\r\n",
                "!apt install ffmpeg\r\n",
                "!apt install xvfb\r\n",
                "\r\n",
                "!pip install cvxpy\r\n",
                "!pip install box2d-py\r\n",
                "!pip uninstall pyglet -y\r\n",
                "!pip uninstall gym -y\r\n",
                "!pip install tensorflow==1.15\r\n",
                "!pip install pyglet==1.3.2\r\n",
                "!pip install gym==0.9.4\r\n",
                "!pip install pyvirtualdisplay"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "<LARGE>Restart the Runtime first before proceeding below</LARGE>"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "!git clone -b paper-training https://github.com/naufalhisyam/rocket-lander.git\r\n",
                "%cd /content/rocket-lander\r\n",
                "!ls"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from pyvirtualdisplay import Display\r\n",
                "\r\n",
                "display = Display(visible=0, size=(1000, 800))\r\n",
                "display.start()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "%tensorflow_version 1.x\r\n",
                "import os\r\n",
                "import numpy as np\r\n",
                "from numpy.core.numeric import False_\r\n",
                "import pandas as pd\r\n",
                "import tensorflow as tf\r\n",
                "\r\n",
                "from control_and_ai.DDPG.ddpg import DDPG\r\n",
                "from control_and_ai.DDPG.utils import Utils\r\n",
                "from control_and_ai.DDPG.exploration import OUPolicy\r\n",
                "\r\n",
                "from constants import *\r\n",
                "from constants import DEGTORAD\r\n",
                "from environments.rocketlander import RocketLander, get_state_sample\r\n",
                "\r\n",
                "action_bounds = [1, 1, 15*DEGTORAD]\r\n",
                "\r\n",
                "eps = []\r\n",
                "eps.append(OUPolicy(0, 0.2, 0.4))\r\n",
                "eps.append(OUPolicy(0, 0.2, 0.4))\r\n",
                "eps.append(OUPolicy(0, 0.2, 0.4))\r\n",
                "\r\n",
                "simulation_settings = {'Side Engines': True,\r\n",
                "                       'Clouds': True,\r\n",
                "                       'Vectorized Nozzle': True,\r\n",
                "                       'Graph': False,\r\n",
                "                       'Render': False,\r\n",
                "                       'Starting Y-Pos Constant': 1,\r\n",
                "                       'Initial Force': 'random',\r\n",
                "                       'Rows': 1,\r\n",
                "                       'Columns': 2,\r\n",
                "                       'Episodes': 500}\r\n",
                "env = RocketLander(simulation_settings)\r\n",
                "\r\n",
                "#Set both line below to False if you want to contniue training from a saved checkpoint\r\n",
                "RETRAIN = True #Restore weights if False\r\n",
                "TEST = False #Test the model\r\n",
                "\r\n",
                "NUM_EPISODES = 300\r\n",
                "SAVE_REWARD = True #Export reward log as .xlsx\r\n",
                "NAME = \"test\" #Model name\r\n",
                "\r\n",
                "model_dir = '/content/gdrive/MyDrive/colab_model/rocket/DDPG/' + NAME\r\n",
                "\r\n",
                "agent = DDPG(\r\n",
                "    action_bounds,\r\n",
                "    eps,\r\n",
                "    env.observation_space.shape[0], #for first model\r\n",
                "    actor_learning_rate=0.0001,\r\n",
                "    critic_learning_rate=0.001,\r\n",
                "    retrain=RETRAIN,\r\n",
                "    log_dir=\"./logs\",\r\n",
                "    model_dir=model_dir,\r\n",
                "    batch_size=100,\r\n",
                "    gamma=0.99)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def train(env, agent):\r\n",
                "    obs_size = env.observation_space.shape[0]\r\n",
                "\r\n",
                "    util = Utils()\r\n",
                "    state_samples = get_state_sample(samples=5000, normal_state=True)\r\n",
                "    util.create_normalizer(state_sample=state_samples)\r\n",
                "    if SAVE_REWARD:\r\n",
                "        rew = []\r\n",
                "        ep = []\r\n",
                "\r\n",
                "    for episode in range(1, NUM_EPISODES + 1):\r\n",
                "        old_state = None\r\n",
                "        done = False\r\n",
                "        total_reward = 0\r\n",
                "\r\n",
                "        state = env.reset()\r\n",
                "        state = util.normalize(state)\r\n",
                "        max_steps = 500\r\n",
                "\r\n",
                "        left_or_right_barge_movement = np.random.randint(0, 2)\r\n",
                "\r\n",
                "        for t in range(max_steps): # env.spec.max_episode_steps\r\n",
                "            old_state = state\r\n",
                "            # infer an action\r\n",
                "            action = agent.get_action(np.reshape(state, (1, obs_size)), not TEST)\r\n",
                "\r\n",
                "            # take it\r\n",
                "            state, reward, done, _ = env.step(action[0])\r\n",
                "            state = util.normalize(state)\r\n",
                "            total_reward += reward\r\n",
                "\r\n",
                "            if state[LEFT_GROUND_CONTACT] == 0 and state[RIGHT_GROUND_CONTACT] == 0:\r\n",
                "                #env.move_barge_randomly(epsilon, left_or_right_barge_movement)\r\n",
                "                env.apply_random_x_disturbance(epsilon=0.005, left_or_right=left_or_right_barge_movement)\r\n",
                "                env.apply_random_y_disturbance(epsilon=0.005)\r\n",
                "\r\n",
                "            if not TEST:\r\n",
                "                # update q vals\r\n",
                "                agent.update(old_state, action[0], np.array(reward), state, done)\r\n",
                "\r\n",
                "            if done:\r\n",
                "                break\r\n",
                "\r\n",
                "        agent.log_data(total_reward, episode)\r\n",
                "\r\n",
                "        if episode % 50 == 0 and not TEST:\r\n",
                "            print('Saved model at episode', episode)\r\n",
                "            agent.save_model(episode)\r\n",
                "        if SAVE_REWARD:\r\n",
                "            rew.append(total_reward)\r\n",
                "            ep.append(episode)\r\n",
                "        print(\"Episode:\\t{0}\\tReward:\\t{1}\".format(episode, total_reward))\r\n",
                "    \r\n",
                "    if SAVE_REWARD:\r\n",
                "        os.makedirs(\"excel_logs/eps-rewards/\", exist_ok=True)\r\n",
                "        reward_data=pd.DataFrame(list(zip(ep,rew)),columns=['episode','reward'])\r\n",
                "        with pd.ExcelWriter(f\"/content/rocket-lander/excel_logs/eps-rewards/DDPG_eps-rewards_{NAME}_{rew[-1]}_{len(ep)}.xlsx\") as writer:\r\n",
                "            reward_data.to_excel(writer, sheet_name=f\"{NAME}_eps-rewards\")\r\n",
                "        !cp -a \"/content/rocket-lander/excel_logs/eps-rewards/.\" \"{model_dir}\""
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "train(env, agent)"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}