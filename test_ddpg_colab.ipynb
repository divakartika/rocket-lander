{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from google.colab import drive\r\n",
    "drive.mount('/content/gdrive')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!apt install python-opengl\r\n",
    "!apt install ffmpeg\r\n",
    "!apt install xvfb\r\n",
    "\r\n",
    "!pip install cvxpy\r\n",
    "!pip install box2d-py\r\n",
    "!pip uninstall pyglet -y\r\n",
    "!pip uninstall gym -y\r\n",
    "!pip install tensorflow==1.15\r\n",
    "!pip install pyglet==1.3.2\r\n",
    "!pip install gym==0.9.4\r\n",
    "!pip install pyvirtualdisplay"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!git clone -b paper-training https://github.com/naufalhisyam/rocket-lander.git\r\n",
    "%cd /content/rocket-lander\r\n",
    "!ls"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Virtual display\r\n",
    "from pyvirtualdisplay import Display\r\n",
    "import glob\r\n",
    "import io\r\n",
    "import base64\r\n",
    "from IPython.display import HTML\r\n",
    "from pyvirtualdisplay import Display\r\n",
    "from IPython import display as ipythondisplay\r\n",
    "\r\n",
    "virtual_display = Display(visible=0, size=(1000, 800))\r\n",
    "virtual_display.start()\r\n",
    "\r\n",
    "def show_video():\r\n",
    "  mp4list = glob.glob('rocket_videos/*.mp4')\r\n",
    "  if len(mp4list) > 0:\r\n",
    "    mp4 = mp4list[0]\r\n",
    "    video = io.open(mp4, 'r+b').read()\r\n",
    "    encoded = base64.b64encode(video)\r\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \r\n",
    "                loop controls style=\"height: 400px;\">\r\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\r\n",
    "             </video>'''.format(encoded.decode('ascii'))))\r\n",
    "  else: \r\n",
    "    print(\"Could not find video\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "%tensorflow_version 1.x\r\n",
    "import numpy as np\r\n",
    "from numpy.core.numeric import False_\r\n",
    "import pandas as pd\r\n",
    "import tensorflow as tf\r\n",
    "\r\n",
    "from control_and_ai.DDPG.ddpg import DDPG\r\n",
    "from control_and_ai.DDPG.utils import Utils\r\n",
    "from control_and_ai.DDPG.exploration import OUPolicy\r\n",
    "\r\n",
    "from constants import *\r\n",
    "from constants import DEGTORAD\r\n",
    "from environments.rocketlander_test import RocketLander, get_state_sample\r\n",
    "\r\n",
    "action_bounds = [1, 1, 15*DEGTORAD]\r\n",
    "\r\n",
    "eps = []\r\n",
    "eps.append(OUPolicy(0, 0.2, 0.4))\r\n",
    "eps.append(OUPolicy(0, 0.2, 0.4))\r\n",
    "eps.append(OUPolicy(0, 0.2, 0.4))\r\n",
    "\r\n",
    "simulation_settings = {'Side Engines': True,\r\n",
    "                       'Clouds': True,\r\n",
    "                       'Vectorized Nozzle': True,\r\n",
    "                       'Graph': False,\r\n",
    "                       'Render': False,\r\n",
    "                       'Starting Y-Pos Constant': 1,\r\n",
    "                       'Initial Force': 'random',\r\n",
    "                       'Rows': 1,\r\n",
    "                       'Columns': 2,\r\n",
    "                       'Episodes': 500}\r\n",
    "\r\n",
    "env = RocketLander(simulation_settings)\r\n",
    "env = wrappers.Monitor(env, './rocket_videos', force=True)\r\n",
    "\r\n",
    "#Set both line below to False if you want to contniue training from a saved checkpoint\r\n",
    "RETRAIN = False#Restore weights if False\r\n",
    "TEST = True #Test the model\r\n",
    "\r\n",
    "NUM_EPISODES = 1\r\n",
    "SAVE_TO_EXCEL = True #Export states & actions logs as .xlsx\r\n",
    "\r\n",
    "NAME = \"test\" #Model name\r\n",
    "\r\n",
    "SIMULATE_WIND = True\r\n",
    "x_force = 2000 # x-axis wind force in Newton\r\n",
    "y_force = 2000 # y-axis wind force in Newton\r\n",
    "\r\n",
    "model_dir = '/content/gdrive/MyDrive/colab_model/rocket/DDPG/' + NAME\r\n",
    "\r\n",
    "agent = DDPG(\r\n",
    "    action_bounds,\r\n",
    "    eps,\r\n",
    "    env.observation_space.shape[0], #for first model\r\n",
    "    actor_learning_rate=0.0001,\r\n",
    "    critic_learning_rate=0.001,\r\n",
    "    retrain=RETRAIN,\r\n",
    "    log_dir=logs,\r\n",
    "    model_dir=model_dir,\r\n",
    "    batch_size=100,\r\n",
    "    gamma=0.99)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def test(env, agent, x_force, y_force):\r\n",
    "    obs_size = env.observation_space.shape[0]\r\n",
    "\r\n",
    "    util = Utils()\r\n",
    "    state_samples = get_state_sample(samples=5000, normal_state=True)\r\n",
    "    util.create_normalizer(state_sample=state_samples)\r\n",
    "    if SAVE_TO_EXCEL:\r\n",
    "        xpos, ypos, xvel, yvel, lander_angle, angular_vel, rem_fuel, lander_mass = ([] for _ in range(8))\r\n",
    "        fE, fS, pSi = ([] for _ in range(3))\r\n",
    "\r\n",
    "    for episode in range(1, NUM_EPISODES + 1):\r\n",
    "        old_state = None\r\n",
    "        done = False\r\n",
    "        total_reward = 0\r\n",
    "\r\n",
    "        state = env.reset()\r\n",
    "        state = util.normalize(state)\r\n",
    "        max_steps = 500\r\n",
    "\r\n",
    "        left_or_right_barge_movement = np.random.randint(0, 2)\r\n",
    "\r\n",
    "        for t in range(max_steps): # env.spec.max_episode_steps\r\n",
    "            old_state = state\r\n",
    "            # infer an action\r\n",
    "            action = agent.get_action(np.reshape(state, (1, obs_size)), not TEST)\r\n",
    "            \r\n",
    "            if SAVE_TO_EXCEL:\r\n",
    "                current_state = env.get_state_with_barge_and_landing_coordinates(untransformed_state=True)\r\n",
    "                \r\n",
    "                xpos.append(current_state[0]-current_state[12]) #xpos_rocket - xpos_landingPad\r\n",
    "                ypos.append(current_state[1]-current_state[13]) #ypos_rocket - ypos_landingPad\r\n",
    "                xvel.append(current_state[2]) #xdot\r\n",
    "                yvel.append(current_state[3]) #ydot\r\n",
    "                lander_angle.append(current_state[4]) #theta\r\n",
    "                angular_vel.append(current_state[5]) #theta_dot\r\n",
    "                rem_fuel.append(current_state[6]) #initial fuel = 0.2 * initial_mass\r\n",
    "                lander_mass.append(current_state[7]) #initial_mass = 25.222\r\n",
    "                \r\n",
    "                fE.append(action[0][0])\r\n",
    "                fS.append(action[0][1])\r\n",
    "                pSi.append(action[0][2])\r\n",
    "\r\n",
    "            # take it\r\n",
    "            state, reward, done, _ = env.step(action[0])\r\n",
    "            state = util.normalize(state)\r\n",
    "            total_reward += reward\r\n",
    "\r\n",
    "            if SIMULATE_WIND:\r\n",
    "                if state[LEFT_GROUND_CONTACT] == 0 and state[RIGHT_GROUND_CONTACT] == 0:\r\n",
    "                    env.apply_random_x_disturbance(epsilon=0.005, left_or_right=left_or_right_barge_movement, x_force=x_force)\r\n",
    "                    env.apply_random_y_disturbance(epsilon=0.005, y_force=y_force)\r\n",
    "\r\n",
    "            if not TEST:\r\n",
    "                # update q vals\r\n",
    "                agent.update(old_state, action[0], np.array(reward), state, done)\r\n",
    "\r\n",
    "            if done:\r\n",
    "                break\r\n",
    "\r\n",
    "        agent.log_data(total_reward, episode)\r\n",
    "\r\n",
    "        if episode % 50 == 0 and not TEST:\r\n",
    "            print('Saved model at episode', episode)\r\n",
    "            agent.save_model(episode)\r\n",
    "        print(\"Episode:\\t{0}\\tReward:\\t{1}\".format(episode, total_reward))\r\n",
    "    \r\n",
    "    if SAVE_TO_EXCEL:\r\n",
    "        state_data=pd.DataFrame(list(zip(xpos,ypos,xvel,yvel,lander_angle,angular_vel,rem_fuel,lander_mass)),\\\r\n",
    "            columns=['x_pos','y_pos','x_vel','y_vel','lateral_angle','angular_velocity','remaining_fuel','lander_mass'])\r\n",
    "        action_data=pd.DataFrame(list(zip(fE,fS,pSi)),columns=['Fe','Fs','Psi'])\r\n",
    "        with pd.ExcelWriter(f\"/content/rocket-lander/excel_logs/states-acts/DDPG_{NAME}_{total_reward}.xlsx\") as writer:\r\n",
    "            state_data.to_excel(writer, sheet_name=\"state\")\r\n",
    "            action_data.to_excel(writer, sheet_name=\"action\")\r\n",
    "        !cp -a \"/content/rocket-lander/excel_logs/states-acts/.\" \"{model_dir}\"\r\n",
    "        !cp -a \"/content/rocket-lander/excel_logs/rocket_videos/.\" \"{model_dir}\"\r\n",
    "        print(\"Logs saved\")\r\n",
    "    env.close()\r\n",
    "    show_video()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test(env, agent, x_force, y_force)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}